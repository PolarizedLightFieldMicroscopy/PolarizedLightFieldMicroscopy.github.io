<HTML>
<HEAD>
<title>Emerging Imaging Techniques at the Intersection of Physics and Data Science</title>
<BASEFONT FACE="Verdana" SIZE="2" />
<META http-equiv=Content-Type content="text/html;charset=utf-8" />
<STYLE>
  body, td { font-family: Verdana; font-size: 10pt; }
  #title a, #footer a { color: #304270; text-decoration: underline; }
  #title a:hover, #footer a:hover { color: blue; }
  #title { font-weight: bold; }
  #date { text-align: right; margin-left: 10px; }
  #author { text-align: right; font-style: italic; }
  #auto { font-style: italic; }
  #manual { font-weight: bold; }
  #note { border: 2px solid #cee0f5; }
  #notebar td, #footer td { color: #304270; padding: 2px; padding-left: 4px; padding-right: 4px; background: #cee0f5; font-size: 8pt; }
  #footer td { font-size: 7pt; }
  #download { text-align: right; margin-left: 10px; }
  #body { padding: 15px; }
  #body h1, #body h2, #body h3 { margin-top: 0px; margin-bottom: 10px; }
  #body h1 { font-size: 1.8em; }
  #body h2 { font-size: 1.5em; }
  #body h3 { font-size: 1.2em; }
</STYLE>
</HEAD>
<BODY>

<!-- Begin note[1798] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [1798] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Emerging Imaging Techniques at the Intersection of Physics and Data Science&nbsp;</td>
  <td id="date">3/25/2022 11:42 AM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GordonConf; GConf; </span><span id="auto">Web Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [1798] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [1798] content begin -->


<H1><FONT size="4">Emerging Imaging Techniques at the Intersection of Physics and Data Science</FONT></H1>
<H1><FONT size="2">Gordon Conference: &nbsp;&nbsp;June 5 - 10, 2022 &nbsp;<A href="https://www.grc.org/image-science-conference/2022">&gt;&gt;</A></FONT></H1>
<H4>Information Extraction via Computational Imaging</H4>
<H4><EM>Each of the people in blue has a section below containing references and notes about them.</EM></H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Matthew Kupinski</STRONG> (University of Arizona, USA)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG><FONT color=#0000ff>Roarke Horstmeyer</STRONG> (Duke University, USA)</FONT></LI>
<LI><STRONG>Sjoerd Stallinga</STRONG> (Delft University of Technology, The Netherlands)</LI>
<UL>
<LI><A href="https://homepage.tudelft.nl/99s1c">https://homepage.tudelft.nl/99s1c/</A><BR><A href="https://www.nature.com/articles/s41467-021-26228-5">Simultaneous orientation and 3D localization microscopy with a Vortex point spread function</A> Nature Communications, 12, 5934, 2021.</LI></UL>
</UL>

<H4>Quantum Imaging</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG><FONT color=#0000ff>George Barbastathis</STRONG> (Massachusetts Institute of Technology, USA)</FONT></LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG>Dan Oron</STRONG> (Weizmann Institute of Science, Israel)</LI>
<LI><STRONG>Nick Vamivakas</STRONG> (University of Rochester, USA)</LI></UL>

<H4>Artificial Intelligence in Image Science</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Chris Dainty</STRONG> (NUI Galway, Ireland)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG><FONT color=#0000ff>George Barbastathis</STRONG> (Massachusetts Institute of Technology, USA)</FONT></LI>
<LI>&nbsp;</LI>
<LI><STRONG><FONT color=#0000ff>Aydogan Ozcan</STRONG> (UCLA, USA)</FONT></LI></UL>

<H4>Multimodal and Biomedical Imaging</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Kyle Myers</STRONG> (FDA, USA)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG>Sophie Brasselet</STRONG> (Institut Fresnel, France)</LI>
<LI><STRONG>Amy Oldenburg</STRONG> (University of North Carolina at Chapel Hill, USA)</LI>
<UL>
<LI>Optical Coherence Tomography</LI></UL>

<LI><STRONG>David Sampson</STRONG> (University of Surrey, United Kingdom)</LI></UL>

<H4>Inverse Problems in Imaging</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Richard Paxman</STRONG> (MDA Information Systems LLC, USA)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG>Mini Das</STRONG> (University of Houston, USA)</LI>
<LI><STRONG><FONT color=#0000ff>Demetri Psaltis</STRONG> (Ecole Polytechnique Fédérale de Lausanne, Switzerland) [Link]</FONT></LI></UL>

<H4>Imaging in Emerging Consumer Applications</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Joyce Farrel</STRONG> (Stanford University, USA)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG>Kaan Aksit</STRONG> (University College of London, United Kingdom)</LI></UL>

<H4>Astronomical and Space Imaging</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Thomas Bifano</STRONG> (Boston University, USA)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG>Jeff Khun</STRONG> (University of Hawaii, USA)</LI>
<LI><STRONG>Meredith Kupinski</STRONG> (University of Arizona, USA)</LI></UL>

<H4>Eyes and Vision</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Jannick Rolland</STRONG> (University of Rochester, USA)</LI></UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG>Kristina Irsch</STRONG> (Johns Hopkins University, USA / Sorbonne University, France)</LI>
<LI><STRONG>Hakan Ürey</STRONG> (Koç University, Turkey)</LI>
<LI><STRONG>Brian Wandell</STRONG> (Stanford University, USA)</LI></UL>

<H4>Novel Lensless Imaging</H4>
<DIV>Discussion Leaders</DIV>
<UL>
<LI><STRONG>Sophie Brasselet</STRONG> (Institut Fresnel, France)</LI>
<BLOCKQUOTE>
<DIV>Polarization !!</DIV>
<DIV><A href="https://www.nature.com/articles/s41467-022-27966-w">4polar-STORM polarized super-resolution imaging of actin filament organization in cells</A></DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=RblOMmsAAAAJ&sortby=pubdate&citation_for_view=RblOMmsAAAAJ:6bLC7aUMtPcC">Polarized microscopy, towards molecular-organization imaging in cells and tissues</A></DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=RblOMmsAAAAJ&sortby=pubdate&citation_for_view=RblOMmsAAAAJ:WC9gN4BGCRcC">Real-time imaging of molecular organization by point scanning nonlinear fast polarization modulation.</A></DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=RblOMmsAAAAJ&sortby=pubdate&citation_for_view=RblOMmsAAAAJ:WC23djZS0W4C">Polarized super resolution imaging of molecular orientations in 3D</A></DIV></BLOCKQUOTE>
</UL>

<DIV>Speakers</DIV>
<UL>
<LI><STRONG><FONT color=#0000ff>Laura Waller</STRONG> (University of California, Berkeley, USA)</FONT></LI></UL>


<!-- Note [1798] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[1798] -->


<!-- Begin note[2666] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2666] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Psaltis&nbsp;</td>
  <td id="date">4/30/2022 2:23 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf; </span><span id="auto">Images; Images:Images with Printed Text</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2666] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2666] content begin -->


<DIV>Psaltis</DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Demetri Psaltis</STRONG> (Ecole Polytechnique Fédérale de Lausanne, Switzerland)</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:anC-K-6lZLwC">Tomographic diffraction microscopy of birefringence</A>. &nbsp;2021</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:Zqvzy0Y0RycC">Polarization-sensitive optical diffraction tomography</A>. &nbsp;2021</DIV>
<DIV><A href="https://www.photoniques.com/articles/photon/abs/2020/05/photon2020104p34/photon2020104p34.html">Optical neural networks: the 3D connection</A></DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:SrJLFjpZNIkC">Deep learning approach for solving the missing cone problem in optical diffraction tomography</A> 2020</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:D8YlkiboQ7MC">Three-dimensional tomography of red blood cells using deep learning</A> 2020</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:wtggH7XvzjUC">MaxwellNet: Physics-driven deep neural network training based on Maxwell’s equations</A>. &nbsp;J Lim, D Psaltis APL Photonics 2021</DIV>
<BLOCKQUOTE>
<DIV>Maxwell’s equations govern light propagation and its interaction with matter. Therefore, the solution of Maxwell’s equations using computational electromagnetic simulations plays a critical role in understanding light–matter interaction and designing optical elements. Such simulations are often time-consuming, and recent activities have been described to replace or supplement them with trained deep neural networks (DNNs). Such DNNs typically require extensive, computationally demanding simulations using conventional electromagnetic solvers to compose the training dataset. In this paper, we present a novel scheme to train a DNN that solves Maxwell’s equations speedily and accurately without relying on other computational electromagnetic solvers. Our approach is to train a DNN using the residual of Maxwell’s equations as the physics-driven loss function for a network that finds the electric field given the</DIV>
<DIV><A href="https://aip-prod-cdn.literatumonline.com/journals/content/app/2022/app.2022.7.issue-1/5.0071616/20220109/suppl/supplementary_material_revised_2nd.pdf?b92b4ad1b4f274c70877518613abb28b424615f7925cf55ec9c2cb79b9a9dfcd69a8987b50485f1f25a8d2d0ec0e0c3337439fc953b7749a8807487592ecfac898bd64d85aa2c9ea51dc5aad7653ad11bc3ef1376645de70b8905995f16812f0ac0c65e112129140d862361578af476a9ad0625f09a0fb9411b64184437d107160451a0f87602dc4a615eaec4eef28971ed0a71db77b6e7193da81a6c2"><STRONG>Supplimental</STRONG></A></STRONG></DIV>
<DIV>Incorps U-Net !!</DIV>
<DIV>&nbsp;</DIV>
<DIV><IMG src="images\0000@2666_2666-412687c78f3dd1bb.png" border="0"></DIV></BLOCKQUOTE>

<DIV>&nbsp;</DIV>
<DIV><STRONG>[!!] <A href="https://www.osapublishing.org/abstract.cfm?uri=optica-8-3-402">Polarization-sensitive optical diffraction tomography</A>. &nbsp;<A href="https://scholar.google.com/citations?user=c_hnbFgAAAAJ&hl=en&oi=sra">A Saba</A>, <A href="https://scholar.google.com/citations?user=Hiv5hH8AAAAJ&hl=en&oi=sra">J Lim</A>, <A href="https://scholar.google.com/citations?user=XjQ2bskAAAAJ&hl=en&oi=sra">AB Ayoub</A>, <A href="https://scholar.google.com/citations?user=e2AexP8AAAAJ&hl=en&oi=sra">EE Antoine</A>, <A href="https://scholar.google.com/citations?user=-CVR2h8AAAAJ&hl=en&oi=sra">D Psaltis</A> - Optica, 2021</STRONG></DIV>
<BLOCKQUOTE>
<DIV align="left">Polarization of light has been widely used as a contrast mechanism in two-dimensional (2D) microscopy and also in some three-dimensional (3D) imaging modalities. In this paper, we report the 3D tomographic reconstruction of the refractive index (RI) tensor using 2D scattered fields measured for different illumination angles and polarizations. Conventional optical diffraction tomography (ODT) has been used as a quantitative, label-free 3D imaging method. It is based on the scalar formalism, which limits its application to isotropic samples. We achieve imaging of the birefringence of 3D objects through a reformulation of ODT based on vector diffraction theory. The off-diagonal components of the RI tensor reconstruction convey additional information that is not available in either conventional scalar ODT or 2D polarization microscopy. Finally, we show experimental reconstructions of 3D objects with a polarization-sensitive contrast metric quantitatively displaying the true birefringence of the samples.</DIV></BLOCKQUOTE>

<DIV><A href="https://www.osapublishing.org/abstract.cfm?uri=COSI-2020-CF4C.5"><STRONG>Deep learning approach for solving the missing cone problem in optical diffraction tomography</A>. </STRONG>&nbsp;Joowon Lim, Ahmed B. Ayoub, and Demetri Psaltis. &nbsp;CF4C.5 Computational Optical Sensing and Imaging (COSI) 2020</DIV>
<BLOCKQUOTE>
<DIV>Optical diffraction tomography (ODT) produces three dimensional distribution of refractive index (RI) by measuring scattering fields at various angles. Although the distribution of RI index is highly informative, due to the missing cone problem stemming from the limited-angle acquisition of holograms, reconstructions have very poor resolution along axial direction compared to the horizontal imaging plane. To solve this issue, here we present a novel unsupervised deep learning framework, which learns the probability distribution of missing projection views through optimal transport driven cycleGAN. Experimental results show that missing cone artifact in ODT can be significantly resolved by the proposed method.</DIV></BLOCKQUOTE>

<DIV align="left">&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>U. S. Kamilov, I. N. Papadopoulos, M. H. Shoreh, A. Goy, C. Vonesch, &nbsp;M. Unser, and D. Psaltis, “Learning approach to optical tomography,” Optica 2, 517–522 (2015).</DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Tomographic diffraction microscopy of birefringence. &nbsp;Amirhossein Saba, Joowon Lim, Ahmed B. Ayoub, Elizabeth E. Antoine, and Demetri Psaltis &nbsp;(Requested...)</STRONG></DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:Zqvzy0Y0RycC">Polarization-sensitive optical diffraction tomography</A>. &nbsp;A Saba, J Lim, AB Ayoub, EE Antoine, D Psaltis &nbsp;Optica 8 (3), 402-408. &nbsp;2021</DIV>
<BLOCKQUOTE>
<DIV>Polarization of light has been widely used as a contrast mechanism in two-dimensional (2D) microscopy and also in some three-dimensional (3D) imaging modalities. In this paper, we report the 3D tomographic reconstruction of the refractive index (RI) tensor using 2D scattered fields measured for different illumination angles and polarizations. Conventional optical diffraction tomography (ODT) has been used as a quantitative, label-free 3D imaging method. It is based on the scalar formalism, which limits its application to isotropic samples. We achieve imaging of the birefringence of 3D objects through a reformulation of ODT based on vector diffraction theory. The off-diagonal components of the RI tensor reconstruction convey additional information that is not available in either conventional scalar ODT or 2D polarization microscopy. Finally, we show experimental reconstructions of 3D objects with a polarization...</DIV></BLOCKQUOTE>


<!-- Note [2666] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2666] -->


<!-- Begin note[2677] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2677] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Horstmeyer, Duke&nbsp;</td>
  <td id="date">4/30/2022 2:39 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf</span><span id="auto"></span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2677] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2677] content begin -->


<DIV><EM><FONT size="4">Horstmeyer, Duke</FONT></EM></DIV>
<H2>Horstmeyer, Duke, Computational Optics Lab</H2>
<P><A href="https://horstmeyer.pratt.duke.edu/people/roarke-horstmeyer">Dr. Roarke Horstmeyer</A>,</P>
<P><A href="http://horstmeyer.pratt.duke.edu">http://horstmeyer.pratt.duke.edu/</A></P>
<P>ML Course: <A href="https://github.com/deepimaging">deepimaging</A>/<A href="https://github.com/deepimaging/deepimaging.github.io">deepimaging.github.io</A></P>
<P><A href="https://deepimaging.io">deepimaging.io</A> &nbsp;<A href="https://horstmeyer.pratt.duke.edu">Computational Optics Lab</A></P>
<P><A href="http://horstmeyer.pratt.duke.edu/publications/convolutional-neural-networks-teach-microscopes-how-image">Convolutional neural networks that teach microscopes how to image</A>. &nbsp;R. Horstmeyer, R. Y. Chen, B. Kappes, and B. Judkewitz. &nbsp;2017</P>
<P>X. Dai et al., &quot;<A href="https://opg.optica.org/boe/fulltext.cfm?uri=boe-13-3-1457&id=469435">Quantitative Jones matrix imaging using vectorial Fourier ptychography</A>,&quot; Biomedical Optics Express (2022).</P>
<BLOCKQUOTE>
<P>This paper presents a microscopic imaging technique that uses variable-angle illumination to recover the complex polarimetric properties of a specimen at high resolution and over a large field-of-view. The approach extends Fourier ptychography, which is a synthetic aperture-based imaging approach to improve resolution with phaseless measurements, to additionally account for the vectorial nature of light. After images are acquired using a standard microscope outfitted with an LED illumination array and two polarizers, our vectorial Fourier ptychography (vFP) algorithm solves for the complex 2x2 Jones matrix of the anisotropic specimen of interest at each resolved spatial location. We introduce a new sequential Gauss-Newton-based solver that additionally jointly estimates and removes polarization-dependent imaging system aberrations. We demonstrate effective vFP performance by generating large-area (29 mm2), high-resolution (1.24 μm full-pitch) reconstructions of sample absorption, phase, orientation, diattenuation, and retardance for a variety of calibration samples and biological specimens.</P>
<P>‘This limits our attention in this work to polarization-dependent eﬀects along <EM>x </EM>and <EM>y</EM>, although future work may extend analysis to examine thicker specimens and eﬀects along <EM>z</EM>.’</P></BLOCKQUOTE>

<P>[!!] <A href="https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-9-12872"><STRONG>Diffraction tomography with a deep image prior</A> &nbsp;</STRONG>Kevin C. Zhou, <EM>Roarke Horstmeyer.</EM> Opt. Express <STRONG>28</STRONG>(9) 12872-12896 (2020)</P>
<BLOCKQUOTE>
<P>We present a tomographic imaging technique, termed Deep Prior Diffraction Tomography (DP-DT), to reconstruct the 3D refractive index (RI) of thick biological samples at high resolution from a sequence of low-resolution images collected under angularly varying illumination. DP-DT processes the multi-angle data using a phase retrieval algorithm that is extended by a deep image prior (DIP), which reparameterizes the 3D sample reconstruction with an untrained, deep generative 3D convolutional neural network (CNN). We show that DP-DT effectively addresses the missing cone problem, which otherwise degrades the resolution and quality of standard 3D reconstruction algorithms. As DP-DT does not require pre-captured data or pre-training, it is not biased towards any particular dataset. Hence, it is a general technique that can be applied to a wide variety of 3D samples, including scenarios in which large datasets for supervised training would be infeasible or expensive.</P>
<P>We further demonstrate the generality of DP-DT, using two different scattering models, the first Born and multi-slice models.</P>
<P>[Deep image prior (DIP)... ] &nbsp;Directly minimizing L can be problematic, given the effects of the missing cone. The DIP is a recently-presented, data-independent method to assist with a large variety of inverse image reconstruction problems without supervised training [34,50–54]. It is an untrained regularization technique that reparameterizes the reconstruction target in the spatial domain as the output of a deep generative CNN that uses pseudorandom noise as input (see Fig. 11 in Appendix A for the architecture we used). After initializing the CNN with pseudorandom noise, DIP optimization then proceeds to update CNN weights to minimize loss, as opposed to directly optimizing the reconstruction target (Smodel). Here, we hypothesize that the DIP’s resistance to unnatural images extends to the third dimension and can help eliminate missing cone artifacts in diffraction tomography.</P></BLOCKQUOTE>

<P><A href="https://github.com/kevinczhou">kevinczhou</A>/<A href="https://github.com/kevinczhou/deep-prior-diffraction-tomography">deep-prior-diffraction-tomography</A>.</P>
<P>X. Yao et al., &quot;<A href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-30-2-1745&id=467320">Increasing a microscope’s effective field of view via overlapped imaging and machine learning</A>,&quot; Optics Express (2022).</P>
<P><A href="https://www.ncbi.nlm.nih.gov/pubmed/24921553">Aperture-scanning Fourier ptychography for 3D refocusing and super-resolution macroscopic imaging.</A> &nbsp;Dong S, Horstmeyer R, Shiradkar R, Guo K, Ou X, Bian Z, Xin H, Zheng G., &nbsp;Opt Express. 2014 Jun 2;22(11):13586-99. doi: 10.1364/OE.22.013586.</P>

<!-- Note [2677] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2677] -->


<!-- Begin note[2690] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2690] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Ozcan&nbsp;</td>
  <td id="date">5/2/2022 1:11 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf</span><span id="auto"></span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2690] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2690] content begin -->


<P><FONT size="4">Ozcan</FONT></P>
<P>Aydogan Ozcan at UCLA</P>
<DIV><A href="http://innovate.ee.ucla.edu">http://innovate.ee.ucla.edu/</A></DIV>
<P><A href="https://innovate.ee.ucla.edu/publications-and-talks.html">https://innovate.ee.ucla.edu/publications-and-talks.html</A></P>
<P>Ozcan, &nbsp;published w/ &nbsp;Barbastathis + <A href="http://www.its.caltech.edu/~ahmet">Ahmet Coskun</A> (with Ozcan) On the use of deep learning for computational imaging. Optica 6, 921–943 (2019)</P>
<P>(One of the co-authors of Bioart with Joe Davis)</P>
<DIV><A href="http://www.opticalgenomics.com">opticalgenomics.com</A></DIV>
<P><A href="http://onlinelibrary.wiley.com/doi/10.1002/adma.201502382/full">Art on the Nanoscale and Beyond</A> Advanced Materials (2016) [with <A href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CB8QFjAA&url=http://albertfolch.wix.com/folchlabintro&ei=OfCWVYLeF8TXoATwvqDYBA&usg=AFQjCNGyprqWde1PA_cudPDSb0nCGvmnxQ&sig2=OYDkBA26TElrHnd94_VL5g">Albert Folch </A>and <A href="http://www2.massgeneral.org/wellman/faculty-yun-pi.htm">Seok Hyun Yun</A>]</P>
<P><STRONG>“Deep learning-based image reconstruction and enhancement in optical microscopy,</STRONG>” &nbsp;K. de Haan, Y. Rivenson, Y. Wu, and A. Ozcan, &nbsp;&nbsp;Proceedings of the IEEE (2019) &nbsp;<A href="https://ieeexplore.ieee.org/document/8901171">DOI: 10.1109/JPROC.2019.2949575</A> – <A href="https://innovate.ee.ucla.edu/wp-content/uploads/2019/11/JPROC2949575.pdf">PDF</A></P>
<P><A href="https://ui.adsabs.harvard.edu/#abs/2020arXiv200700741L/abstract"><STRONG>Deep learning-based holographic polarization microscopy</A>. </STRONG>&nbsp;Ozcan, et al. July 2020</P>
<BLOCKQUOTE>
<P>Polarized light microscopy provides high contrast to birefringent specimen and is widely used as a diagnostic tool in pathology. However, polarization microscopy systems typically operate by analyzing images collected from two or more light paths in different states of polarization, which lead to relatively complex optical designs, high system costs or experienced technicians being required. Here, we present a deep learning-based holographic polarization microscope that is capable of obtaining quantitative birefringence retardance and orientation information of specimen from a phase recovered hologram, while only requiring the addition of one polarizer/analyzer pair to an existing holographic imaging system. Using a deep neural network, the reconstructed holographic images from a single state of polarization can be transformed into images equivalent to those captured using a single-shot computational polarized light microscope (SCPLM). Our analysis shows that a trained deep neural network can extract the birefringence information using both the sample specific morphological features as well as the holographic amplitude and phase distribution. To demonstrate the efficacy of this method, we tested it by imaging various birefringent samples including e.g., monosodium urate (MSU) and triamcinolone acetonide (TCA) crystals. Our method achieves similar results to SCPLM both qualitatively and quantitatively, and due to its simpler optical design and significantly larger field-of-view, this method has the potential to expand the access to polarization microscopy and its use for medical diagnosis in resource limited settings.</P></BLOCKQUOTE>

<P>&nbsp;</P>
<P><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=4MR2wSIAAAAJ&sortby=pubdate&citation_for_view=4MR2wSIAAAAJ:ZeXyd9-uunAC">Computational imaging, sensing and diagnostics for global health applications</A> 2014 AF Coskun, A Ozcan, Current opinion in biotechnology 25, 8-16</P>
<P><A href="file://F:/GBHData/My EverNote Files/DataBases/Exported/PLM2/Democratization of Nanoscale Imaging and Sensing Tools Using Photonics">Democratization of Nanoscale Imaging and Sensing Tools Using Photonics</A></P>
<P>“Deep learning enhanced mobile-phone microscopy,”( 2017) <A href="https://arxiv.org/abs/1712.04139" target="_blank">arXiv:1712.04139</A></P>
<P>“Deep Learning Microscopy,” Optica (2017) <A href="https://doi.org/10.1364/OPTICA.4.001437" target="_blank">DOI: 10.1364/OPTICA.4.001437</A> – <A href="http://innovate.ee.ucla.edu/wp-content/uploads/2017/06/OzcanGroup-Optica-2017wSI.pdf" target="_blank">PDF</A></P>
<P><A href="https://arxiv.org/abs/1609.05158">[1609.05158] Real-Time Single Image and Video Super ...</A></P>
<P><A href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</A></P>
<P>“Phase recovery and holographic image reconstruction using deep learning in neural networks,” Light: Science &amp; Applications (Nature Publishing Group) (2017) &nbsp;<A href="http://aap.nature-lsa.cn:8080/cms/accessory/files/AAP-lsa2017141.pdf" target="_blank">DOI: 10.1038/lsa.2017.141</A> – <A href="http://innovate.ee.ucla.edu/wp-content/uploads/2010/03/Ozcan-Group-CNN-LSA-2017-wSI.pdf" target="_blank">PDF</A></P>
<P>E. McLeod and A. Ozcan, “Microscopy without lenses,” Physics Today (2017) <A href="http://dx.doi.org/10.1063/PT.3.3693" target="_blank">DOI: 10.1063/PT.3.3693</A> – <A href="http://innovate.ee.ucla.edu/wp-content/uploads/2010/03/Microscopy-Lensless-Physics-Today-2017.pdf" target="_blank">PDF</A></P>
<P><A href="https://arxiv.org/pdf/1705.04286">Phase recovery and holographic image reconstruction using ...</A> deep learning in neural networks, Ozcan, et al.</P>
<P>&nbsp;</P>
<P><STRONG>“PhaseStain: The digital staining of label-free quantitative phase microscopy images using deep learning,” </STRONG>Y. Rivenson, T. Liu, Z. Wei, Y. Zhang, K. de Haan, and A. Ozcan, Light: Science &amp; Applications (Nature) (2019) <A href="https://rdcu.be/bkvyN">DOI: 10.1038/s41377-019-0129-y</A> – <A href="https://innovate.ee.ucla.edu/wp-content/uploads/2019/03/2019-phasestain.pdf">PDF</A></P>
<P><STRONG>Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning<A href="https://www.nature.com/articles/s41551-019-0362-y">&gt;&gt;</A></STRONG></P>
<P><STRONG>“Pathological crystal imaging with single-shot computational polarized light microscopy,” </STRONG>B. Bai, H. Wang, T. Liu, Y. Rivenson, J. FitzGerald, and A. Ozcan, Journal of Biophotonics (2019) <A href="https://doi.org/10.1002/jbio.201960036">DOI: 10.1002/jbio.201960036</A></P>
<P><A href="https://www.nature.com/articles/s41377-019-0139-9">Bright-field holography: cross-modality deep learning ...</A></P>
<P>“Air Quality Monitoring Using Mobile Microscopy and Machine Learning,” Light: Science &amp; Applications (Nature Publishing Group) (2017) <A href="http://rdcu.be/vGcK" target="_blank">DOI: 10.1038/lsa.2017.46</A> – <A href="http://innovate.ee.ucla.edu/wp-content/uploads/2010/03/2017-airquality.pdf" target="_blank">PDF</A></P>
<P>”Unconventional methods of imaging: computational microscopy and compact implementations,” Reports on Progress in Physics (2016) <A href="http://iopscience.iop.org/article/10.1088/0034-4885/79/7/076001/meta" target="_blank">DOI:10.1088/0034-4885/79/7/076001</A> – <A href="http://innovate.ee.ucla.edu/wp-content/uploads/2016/05/ROPP-2016-Ozcan-McLeod.pdf" target="_blank">PDF</A></P>

<!-- Note [2690] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2690] -->


<!-- Begin note[2698] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2698] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Waller&nbsp;</td>
  <td id="date">5/2/2022 2:09 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf; </span><span id="auto">Word Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2698] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2698] content begin -->


<H1>Waller</H1>
<H2>Waller, UC Berkley</H2>
<P><A href="https://www.laurawaller.com/publications/laura-waller">Laura Waller » Computational Imaging Lab</A> &amp; <A href="https://people.eecs.berkeley.edu/~kellman">Michael Kellman</A> UC Berkley</P>
<P><A href="https://www.laurawaller.com/publications/laura-waller">https://www.laurawaller.com/publications/laura-waller/</A></P>
<P>&nbsp;</P>
<P><A href="https://www.osapublishing.org/abstract.cfm?uri=oe-28-22-32266"><STRONG>Roadmap on 3D integral imaging: sensing, processing, and ...</A> Laura Waller,..., Gordon Wetzstein, &nbsp;2020</STRONG></P>
<P><A href="https://www.osapublishing.org/abstract.cfm?URI=3D-2017-DW2F.2">Modeling light propagation in 3D phase objects</A></P>
<BLOCKQUOTE>
<P>Accurate and fast simulation of light propagating through 3D biological cells is important to the development of new computational imaging systems. We compare the finite-difference time-domain (FDTD), multislice, first Born approximation, and series-expanded Born (SEAGLE) simulation methods.</P></BLOCKQUOTE>

<DIV><STRONG>Multi-layer Born multiple-scattering model for 3D phase microscopy. &nbsp;</STRONG>Michael Chen, David Ren, Hsiou-Yuan Liu, Shwetadwip Chowdhury, and Laura Waller &nbsp;<A href="https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-7-5-394&id=431219">&gt;&gt;</A></DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Single-shot diffuser-encoded light field imaging. &nbsp;&nbsp;Ng, Waller. &nbsp;2016 IEEE &nbsp;<A href="https://ieeexplore.ieee.org/document/7492880">Go to source web page&gt;&gt;</A></STRONG></DIV>
<BLOCKQUOTE>
<DIV>We capture 4D light field data in a single 2D sensor image by encoding spatio-angular information into a speckle field (causticpattern) through a phase diffuser. Using wave-optics theory and a coherent phase retrieval method, we calibrate the system by measuring the diffuser surface height from through-focus images. Wave-optics theory further informs the design of system geometry such that a purely additive ray-optics model is valid. Light field reconstruction is done using nonlinear matrix inversion methods, including ℓ1 minimization. We demonstrate a prototype system and present empirical results of 4D light field reconstruction and computational refocusing from a single diffuser-encoded 2D image.</DIV></BLOCKQUOTE>

<P>&nbsp;</P>
<P><A href="https://people.eecs.berkeley.edu/~kellman/topics/pbld2018/BAIR2018_final.pdf">Data-Driven Design for Computational Imaging</A></P>
<P><A href="https://doi.org/10.1117/12.2543402"><STRONG>Physics-based learning for measurement diversity in 3D refractive index microscopy</A>,</STRONG> 2020 SPIE</P>
<BLOCKQUOTE>
<P>3D refractive index imaging methods usually rely on a weak-scattering approximation that does not allow for thick samples to be imaged accurately. Recent methods such as 3D Fourier ptychographic microscopy (FPM) instead use multiple-scattering models which allow for thicker objects to be imaged. In practice the illumination-side coding of 3D FPM requires redundant information and may produce inaccurate reconstructions for thick samples. Here, we propose augmenting 3D FPM with detection-side coding using a spatial light modulator (SLM) and optimize the SLM coding strategy with physics-based machine learned pupil coding designs that are optimized for 3D reconstructions. We compare our learned designs to random-, defocus-, Zernike aberrations-based pupil codes in simulated and experimental results.</P></BLOCKQUOTE>

<P><A href="https://arxiv.org/abs/2001.09803">Deep Phase Decoder: Self-calibrating phase microscopy with an untrained deep neural network</A>. &nbsp;Emrah Bostan, Reinhard Heckel, Michael Chen, Michael Kellman, Laura Waller. &nbsp;2020</P>
<BLOCKQUOTE>
<P>Deep neural networks have emerged as effective tools for computational imaging including quantitative phase microscopy of transparent samples. To reconstruct phase from intensity, current approaches rely on supervised learning with training examples; consequently, their performance is sensitive to a match of training and imaging settings. Here we propose a new approach to phase microscopy by using an untrained deep neural network for measurement formation, encapsulating the image prior and imaging physics. Our approach does not require any training data and simultaneously reconstructs the sought phase and pupil-plane aberrations by fitting the weights of the network to the captured images. To demonstrate experimentally, we reconstruct quantitative phase from through-focus images blindly (i.e. no explicit knowledge of the aberrations).</P></BLOCKQUOTE>

<P><A href="https://arxiv.org/abs/1808.03571">&quot;Physics-based Learned Design: Optimized Coded-Illumination for Quantitative Phase Imaging.&quot; &nbsp;</A>Michael Kellman, Emrah Bostan, Nicole Repina, Laura Waller. &nbsp;Transactions on Computational Imaging. IEEE, 2019.</P>
<BLOCKQUOTE>
<P>Coded-illumination can enable quantitative phase microscopy of transparent samples with minimal hardware requirements. Intensity images are captured with different source patterns and a non-linear phase retrieval optimization reconstructs the image. The non-linear nature of the processing makes optimizing the illumination pattern designs complicated. Traditional techniques for experimental design (e.g. condition number optimization, spectral analysis) consider only linear measurement formation models and linear reconstructions. Deep neural networks (DNNs) can efficiently represent the non-linear process and can be optimized over via training in an end-to-end framework. However, DNNs typically require a large amount of training examples and parameters to properly learn the phase retrieval process, without making use of the known physical models. Here, we aim to use both our knowledge of the physics and the power of machine learning together. We develop a new data-driven approach to optimizing coded-illumination patterns for a LED array microscope for a given phase reconstruction algorithm. Our method incorporates both the physics of the measurement scheme and the non-linearity of the reconstruction algorithm into the design problem. This enables efficient parameterization, which allows us to use only a small number of training examples to learn designs that generalize well in the experimental setting without retraining. We show experimental results for both a well-characterized phase target and mouse fibroblast cells using coded-illumination patterns optimized for a sparsity-based phase reconstruction algorithm. Our learned design results using 2 measurements demonstrate similar accuracy to Fourier Ptychography with 69 measurements.</P></BLOCKQUOTE>

<P>&nbsp;</P>
<P><A href="http://www.opticsexpress.org/abstract.cfm?URI=oe-29-13-20913">Untrained networks for compressive lensless photography</A>. &nbsp;Kristina Monakhova; Vi Tran; Grace Kuo; Laura Waller. &nbsp;Opt. Express, 29 (13), pp. 20913–20929, 2021.</P>
<P><IMG src="images\0000@2698_clip_image001.jpg" border="0" width="168" height="168" alt="Article Cover"></P>
<P><A href="https://github.com/Waller-Lab/UDN">https://github.com/Waller-Lab/UDN/</A></P>
<P>&nbsp;</P>
<P><A href="https://arxiv.org/abs/2003.05551">Memory-efficient Learning for Large-scale Computational Imaging</A> 2020</P>
<BLOCKQUOTE>
<P>Critical aspects of computational imaging systems, such as experimental design and image priors, can be optimized through deep networks formed by the <STRONG>unrolled iterations of classical model-based reconstructions</STRONG> (termed<STRONG> physics-based</STRONG> networks). However, for real-world large-scale inverse problems, computing gradients via backpropagation is infeasible due to memory limitations of graphics processing units. In this work, we propose a memory-efficient learning procedure that exploits the reversibility of the network's layers to enable data-driven design for large-scale computational imaging systems. We demonstrate our method on a small-scale <STRONG>compressed sensing</STRONG> example, as well as two large-scale real-world systems: multi-channel magnetic resonance imaging and super-resolution optical microscopy.</P></BLOCKQUOTE>

<P>&nbsp;</P>
<P><A href="https://doi.org/10.1038/s41598-019-43845-9">Low-cost, sub-micron resolution, wide-field computational microscopy using opensource hardware</A></P>
<P>&nbsp;</P>
<P><A href="https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-6-9-1211&id=418751#ref34">High-resolution 3D refractive index microscopy of multiple-scattering samples from intensity images</A>. &nbsp;Waller, 2019</P>
<P>&nbsp;</P>
<P><A href="https://www.osapublishing.org/josk/abstract.cfm?uri=josk-18-6-691">High-Resolution 3-D Refractive Index Tomography and 2-D ...</A> &nbsp;2014</P>
<P>&nbsp;</P>
<P><A href="https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-2-2-104&id=310823">3D intensity and phase imaging from light field measurements in an LED array microscope</A>. &nbsp;Lei Tian and Laura Waller &nbsp;2015</P>
<P>&nbsp;</P>
<DIV><A href="https://arxiv.org/abs/2005.13531"><STRONG><FONT size="3">How to do Physics-based Learning</A> </FONT>&nbsp;Kellman, Lustig, Waller, 2020</STRONG></DIV>
<BLOCKQUOTE>
<DIV>The goal of this tutorial is to explain step-by-step how to implement physics-based learning for the rapid prototyping of a computational imaging system. We provide a basic overview of physics-based learning, the construction of a physics-based network, and its reduction to practice. Specifically, we advocate exploiting the auto-differentiation functionality twice, once to build a physics-based network and again to perform physics-based learning. Thus, the user need only implement the forward model process for their system, speeding up prototyping time. We provide an open-source Pytorch implementation of a physics-based network and training procedure for a generic sparse recovery problem.</DIV></BLOCKQUOTE>

<H1><A href="https://github.com/kellman"><FONT size="2">kellman</A>/<A href="https://github.com/kellman/physics_based_learning">physics_based_learning</A></FONT></H1>

<!-- Note [2698] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2698] -->


<!-- Begin note[2701] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2701] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Barbastathis, MIT&nbsp;</td>
  <td id="date">5/2/2022 2:13 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf</span><span id="auto"></span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2701] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2701] content begin -->


<H2>Barbastathis, MIT</H2>
<P><A href="http://optics.mit.edu">Welcome to 3D Optical Systems Group at MIT! | 3D Optical ...</A></P>
<P>George Barbastathis<A href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ&view_op=list_works&sortby=pubdate">&gt;&gt;</A> GoogScholar</P>
<P><A href="https://becominghuman.ai/using-ai-for-computational-imaging-applications-8ff390307e65">Using AI For Computational Imaging Applications - Becoming ...</A></P>
<P><A href="https://on-demand.gputechconf.com/gtc/2018/presentation/S8242-Yang-Juntao-paper.pdf">Deep Learning for Computational Science and Engineering</A></P>
<P><STRONG>On the use of deep learning for computational imaging. &nbsp;</STRONG>George Barbastathis, Aydogan Ozcan, and Guohai Situ, Optica, 6(8), 921-943 (2019). <A href="https://www.osapublishing.org/optica/abstract.cfm?uri=optica-6-8-921">link</A></P>
<P>&nbsp;</P>
<P><A href="https://arxiv.org/abs/1702.08516">Lensless computational imaging through deep learning - arXiv</A></P>
<P><A href="https://dspace.mit.edu/handle/1721.1/122070">Computational imaging through deep learning</A> MI Thesis</P>
<BLOCKQUOTE>
<P>In this thesis, the application of DL approaches in two diﬀerent CI scenarios are investigated: imaging through a glass diﬀuser and quantitative phase retrieval (QPR), where an Imaging through Diﬀuser Network (IDiﬀNet) and a Phase Extraction Neural Network (PhENN) are experimentally demonstrated, respectively. This thesis also studies the inﬂuences of the two main factors that determine the performance of a trained neural network: network architecture (connectivity, network depth, etc) and training example quality (spatial frequency content in particular). Motivated by the analysis of the latter factor, two novel approaches, spectral pre-modulation approach and Learning Synthesis by DNN (LS-DNN) method, are successively proposed to improve the visual qualities of the network outputs. &nbsp;Finally, the LS-DNN enhanced PhENN is applied to a phase microscope to recover the phase of a red blood cell (RBC) sample. Furthermore, through simulation of the learned weak object transfer function (WOTF) and experiment on a star-like phase target, we demonstrate that our network has indeed learned the correct physical model rather than doing something trivial as pattern matching.</P></BLOCKQUOTE>

<P>&nbsp;</P>
<P><STRONG>High-resolution limited-angle phase tomography of dense layered objects using deep neural networks.</STRONG> &nbsp;Alexandre Goy, Girish Rughoobur, Shuai Li, Kwabena Arthur, Akintunde Akinwande, and George Barbastathis, &nbsp;Proceedings of the National Academy of Sciences, 116 (40) 19848-19856 (2019). <A href="https://www.pnas.org/content/116/40/19848.short">link</A></P>
<BLOCKQUOTE>
<P><A name="_Hlk84337875">a DNN trained only on synthetic data can be used to successfully reconstruct physical samples disjoint from the synthetic training set. Thus, the need for producing a large number of physical examples for training is ameliorated. The method is generally applicable to tomography with electromagnetic or other types of radiation at all bands.</A></P></BLOCKQUOTE>

<P>&nbsp;</P>
<P><A href="https://www.nature.com/articles/s41377-020-0302-3">Phase imaging with an untrained neural network</A>. &nbsp;<A href="https://www.nature.com/articles/s41377-020-0302-3#auth-7">George Barbastathis</A> et al &nbsp;<A href="https://www.nature.com/lsa">Light: Science &amp; Applications</A> volume 9, Article number: 77 (2020)</P>
<BLOCKQUOTE>
<P>it is possible to experimentally recover an image with an untrained neural network that is built by combining a conventional artificial network such as U-Net19 with a real-world physical model that represents the image formation physics; we call the resulting model PhysenNet.</P></BLOCKQUOTE>

<P><A href="https://ui.adsabs.harvard.edu/#abs/2020arXiv200710734K/abstract">Limited-angle tomographic reconstruction of dense layered objects by dynamical machine learning. </A>Barbastathis &nbsp;2020</P>
<BLOCKQUOTE>
<P>The number of measurements needed for tomographic reconstruction was largely reduced with the use of Deep Neural Network. Therefore, high-resolution 3D reconstruction was possible with only 20 degree of scanning. <A href="http://optics.mit.edu/research">&gt;&gt;</A></P></BLOCKQUOTE>

<P><A href="https://arxiv.org/abs/1711.06810">Imaging through glass diffusers using densely connected convolutional networks</A> &nbsp;Shuai Li, Mo Deng, Justin Lee, Ayan Sinha, George Barbastathis &nbsp;2017</P>
<BLOCKQUOTE>
<P>Computational imaging through scatter generally is accomplished by first characterizing the scattering medium so that its forward operator is obtained; and then imposing additional priors in the form of regularizers on the reconstruction functional so as to improve the condition of the originally ill-posed inverse problem. In the functional, the forward operator and regularizer must be entered explicitly or parametrically (e.g. scattering matrices and dictionaries, respectively.) However, the process of determining these representations is often incomplete, prone to errors, or infeasible. Recently, deep learning architectures have been proposed to instead learn both the forward operator and regularizer through examples. Here, we propose for the first time, to our knowledge, a convolutional neural network architecture called &quot;IDiffNet&quot; for the problem of imaging through diffuse media and demonstrate that IDiffNet has superior generalization capability through extensive tests with well-calibrated diffusers. We found that the Negative Pearson Correlation Coefficient loss function for training is more appropriate for spatially sparse objects and strong scattering conditions. Our results show that the convolutional architecture is robust to the choice of prior, as demonstrated by the use of multiple training and testing object databases, and capable of achieving higher space-bandwidth product reconstructions than previously reported.</P>
<P><IMG src="images\0000@2701_clip_image002.jpg" border="0" width="419" height="305"></P></BLOCKQUOTE>

<P>&nbsp;</P>
<P>J. C. Petruccelli, L. Tian, G. Barbastathis, The transport of intensity equation for optical path length recovery using partially coherent illumination. Opt. Exp. 21, 14430 (2013)</P>
<P>&nbsp;</P>
<P><A href="https://dspace.mit.edu/handle/1721.1/67600">High-resolution 3-D refractive index imaging and Its ...</A> Sung, 2011</P>
<BLOCKQUOTE>
<P>This thesis presents a theory of 3-D imaging in partially coherent light under a non-paraxial condition. The transmission cross-coefficient (TCC) has been used to characterize partially coherent imaging in a 2- D and 3-D paraxial model. It is shown that the 3-D TCC under the non-paraxial condition is an overlap integral of three hemispheres or 3-D pupil functions. By an inspection of the overlap integral, a simple formula is derived that can be efficiently calculated by multiple applications of the 3-D fast Fourier transform (FFT) instead of a 6-D integral. The theory is applied to phase contrast and differential interference contrast (DIC), and it provides the most rigorous 3-D model that has ever been suggested. Contrast-agent-free microscopy is highly desirable to study the dynamics and physiological activity of various structures in living cells. Refractive index is an intrinsic contrast source, but at the same time it is an important biochemical parameter that is proportional to the concentration of molecules. By measuring the refractive index quantitatively, the alteration of cells under chemicals or drugs as well as their normal physiological activities can be monitored in most native conditions. This thesis presents 3-D optical diffraction tomography (ODT) to retrieve the 3-D refractive index map of a transparent biological sample and applies them to some interesting biological problems: the study of cell growth and monitoring the effect of drugs on multiple myeloma cells. In most practical applications of ODT, the angular coverage of an incident beam is limited due to finite system numerical aperture. The refractive index map reconstructed from the restricted data set suffers from the missing cone artifact: elongation of the reconstructed shape along the optical axis and underestimation of the value of the refractive index. This thesis presents that the missing-cone artifact can be effectively suppressed by applying positivity and piecewise-smoothness constraints in the iterative reconstruction framework. By filling the missing cone, a 3-D refractive index map can be reconstructed from the scattering into the entire 4-[pi]nt solid angle. With the improved accuracy, we attempt to quantify the dry mass of chromosomes in single living cells in their mitotic phase.</P></BLOCKQUOTE>


<!-- Note [2701] content end -->

</td></tr>
</table>

</td></tr>
</table>

<!-- End note[2701] -->


</BODY>
</HTML>
 