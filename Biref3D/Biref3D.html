<HTML>
<HEAD>
<title>uPTI: uniaxial permittivity tensor imaging of intrinsic density and anisotropy Shalin</title>
<BASEFONT FACE="Verdana" SIZE="2" />
<META http-equiv=Content-Type content="text/html;charset=utf-8" />
<STYLE>
  body, td { font-family: Verdana; font-size: 10pt; }
  #title a, #footer a { color: #304270; text-decoration: underline; }
  #title a:hover, #footer a:hover { color: blue; }
  #title { font-weight: bold; }
  #date { text-align: right; margin-left: 10px; }
  #author { text-align: right; font-style: italic; }
  #auto { font-style: italic; }
  #manual { font-weight: bold; }
  #note { border: 2px solid #cee0f5; }
  #notebar td, #footer td { color: #304270; padding: 2px; padding-left: 4px; padding-right: 4px; background: #cee0f5; font-size: 8pt; }
  #footer td { font-size: 7pt; }
  #download { text-align: right; margin-left: 10px; }
  #body { padding: 15px; }
  #body h1, #body h2, #body h3 { margin-top: 0px; margin-bottom: 10px; }
  #body h1 { font-size: 1.8em; }
  #body h2 { font-size: 1.5em; }
  #body h3 { font-size: 1.2em; }
</STYLE>
</HEAD>
<BODY>

<!-- Begin note[2185] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2185] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">uPTI: uniaxial permittivity tensor imaging of intrinsic density and anisotropy Shalin&nbsp;</td>
  <td id="date">3/19/2022 2:19 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">MachineLearning; PLM; PolScope; Birefringence3D; </span><span id="auto">Images; Images:Images with Printed Text</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2185] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2185] content begin -->


<H2><A href="https://www.biorxiv.org/content/10.1101/2020.12.15.422951v1"><FONT size="2">uPTI: uniaxial permittivity tensor imaging of intrinsic density and anisotropy</A> &nbsp;&nbsp;Shalin B. Mehta &nbsp;2020</FONT></H2>
<H2><A href="https://scholar.google.com/citations?user=WqlCwNYAAAAJ&sortby=pubdate"><FONT size="2">‪Shalin Mehta‬ - ‪Google Scholar‬</A></FONT></H2>
<DIV><A href="https://www.biorxiv.org/content/10.1101/631101v3"><STRONG>Revealing architectural order with quantitative label-free ...</STRONG></A></STRONG></DIV>
<UL>
<LI><P><A href="https://github.com/mehta-lab/reconstruct-order">https://github.com/mehta-lab/reconstruct-order</A></P></LI>
<LI><P><A href="https://github.com/czbiohub/microdl">https://github.com/czbiohub/microdl</A></P></LI></UL>

<H2><A href="https://www.mehta.science"><FONT size="2">Computational Microscopy, CZ Biohub - Mehta Lab</A></FONT></H2>
<H2><A href="https://github.com/mehta-lab"><FONT size="2">mehta-lab</A>/<A href="https://github.com/mehta-lab/microDL">microDL</A> U-Net</FONT></H2>
<DIV><STRONG>Mehta... uPTI: uniaxial permittivity tensor imaging of intrinsic density and anisotropy<A href="https://www.biorxiv.org/content/10.1101/2020.12.15.422951v1">&gt;&gt;</A> &nbsp;Shalin B. Mehta &nbsp;2020</STRONG></DIV>
<DIV>&nbsp;</DIV>
<DIV><IMG src="images\0002@2185_0001@2185_412683ebcf500611-0894.png" border="0"></DIV>
<DIV>&nbsp;</DIV>
<DIV>S. B. Mehta and C. J. Sheppard, “Quantitative phase-gradient imaging at high resolution with asymmetric illumination-based differential phase contrast,” Opt. Lett. <STRONG>34</STRONG>, 1924–1926 (2009). [<A href="http://dx.doi.org/10.1364/OL.34.001924" target="_blank">CrossRef</A>] &nbsp;&nbsp;[<A href="http://www.ncbi.nlm.nih.gov/pubmed/19571953" target="_blank">PubMed</A>] &nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Polarized light microscopy for 3-dimensional mapping of collagen fiber architecture in ocular tissues. &nbsp;</STRONG>Yang B, Jan NJ, Brazile B, Voorhees A, Lathrop KL, Sigal IA. 2018.</DIV>
<DIV>Journal of Biophotonics 11:e201700356. DOI: https:// doi.org/10.1002/jbio.201700356, PMID: 29633576</DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>DynaMorph: self-supervised learning of morphodynamic states of live cells. Wu Z, Chhun BB, Popova G, Guo SM, Kim CN, Yeh LH, Nowakowski T, Zou J, Mehta SB. Mol Biol Cell. 2022 .</STRONG></DIV>
<H1><A href="https://github.com/mehta-lab"><FONT size="2">mehta-lab</A>/<A href="https://github.com/mehta-lab/reconstruct-order">reconstruct-order</A></FONT></H1>
<DIV><IMG src="images\0000@2185_0000@106117_0-4126872b3db363bf.png" border="0"></DIV>
<DIV>Methods A: Label-free brightfield, phase, and retardance imaging. We acquired all data using a Leica DMI-8 inverted widefield microscope, with 20x objective magnification at 0.55NA (air), and 0.4NA condenser on a Hamamatsu Flash-4 LT camera (6.5 um pixels). The cells were held at a constant 37°C, 5% CO2 using the Okolab stage-top incubator (H101-K-Frame). Each of the five polarization states were acquired, in sequence, with 50 ms camera exposure, for each of 5 z-planes for a given field of view. For the unperturbed microglia time series (training set), we acquired 52 time points and 27 fields of view (9 per well) over 24 hours at 27 minute time intervals. For the perturbed microglia time series (test set), we acquired 159 time points, 9 fields of view (4 were used for analysis), over 24 hours at 9 minute time intervals. Birefringence is an optical property of matter that describes a different refractive index for different orientation axes of polarized light. The differential phase shift caused by these refractive indices, and the orientation axes, allow us to decompose birefringence into two properties: retardance and orientation (respectively). Each of these properties can be represented in terms of a combination of the Stokes parameters, which are Cartesian projections of a vector from spherical coordinates that describes the full polarization state of light. With the application of Mueller matrices that are tuned to our instrument setup, we can translate the Stokes representation into image intensities and vice versa. Therefore, given a set of intensity images of the specimen with known polarization states, we can use the inverse model to compute the sample’s corresponding Stokes parameters, and from there compute physical properties of retardance and orientation. We estimate the background level of polarization with a 2D polynomial fit. Details for label-free hardware calibration, background correction and reconstruction can be found in previous work (Guo et al., 2020). We used the open-source microscope control software Micro-Manager 1.4.22 (<A href="https://micro-manager.org">https://micro-manager.org/</A>) for all image acquisition. The Micro-Manager plugin OpenPolScope (<A href="https://openpolscope.org">https://openpolscope.org/</A>) performs the liquid-crystal-compensator (LCC) calibration by first finding voltages to achieve extinction (IExt , intensity minimum). A pre-defined &quot;Swing&quot; voltage (0.03) is applied to induce slight polarization ellipticity along one axis (I0deg). This &quot;Swing&quot; is empirically determined based on the sample, in order to produce maximum polarization contrast. Then, using the brent-optimizer minimization procedure, we find three more voltage states centered on I0deg that sample other orientations (I45deg, I90deg, I135deg). Following the reconstruction algorithm described previously (Guo et al., 2020) and documented on github (<A href="https://github.com">https://github.com/</A> mehta-lab/reconstruct-order), phase reconstructions used the above hardware parameters plus the total variation regularizer with parameters: rho=1,itr=50,absorption=1.0e-3,phase=1.0e-5. Retardance reconstructions used default parameters plus &quot;local fit&quot; background correction, and retardance scaling of 1e4.</DIV>

<!-- Note [2185] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2185] -->


<!-- Begin note[2666] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2666] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Psaltis&nbsp;</td>
  <td id="date">4/30/2022 2:23 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf; Birefringence3D; </span><span id="auto">Images; Images:Images with Printed Text</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2666] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2666] content begin -->


<DIV><FONT size="4">Psaltis</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Demetri Psaltis</STRONG> (Ecole Polytechnique Fédérale de Lausanne, Switzerland)</DIV>
<DIV>&nbsp;</DIV>
<DIV>imaging of the birefringence of 3D objects through a reformulation of ODT based on vector diffraction theory</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://www.osapublishing.org/abstract.cfm?uri=optica-8-3-402"><STRONG>Polarization-sensitive optical diffraction tomography</A>. &nbsp;<A href="https://scholar.google.com/citations?user=c_hnbFgAAAAJ&hl=en&oi=sra">A Saba</A>, <A href="https://scholar.google.com/citations?user=Hiv5hH8AAAAJ&hl=en&oi=sra">J Lim</A>, <A href="https://scholar.google.com/citations?user=XjQ2bskAAAAJ&hl=en&oi=sra">AB Ayoub</A>, <A href="https://scholar.google.com/citations?user=e2AexP8AAAAJ&hl=en&oi=sra">EE Antoine</A>, D Psaltis - Optica, 2021</STRONG></DIV>
<BLOCKQUOTE>
<DIV align="left">Polarization of light has been widely used as a contrast mechanism in two-dimensional (2D) microscopy and also in some three-dimensional (3D) imaging modalities. In this paper, we report the 3D tomographic reconstruction of the refractive index (RI) tensor using 2D scattered fields measured for different illumination angles and polarizations. Conventional optical diffraction tomography (ODT) has been used as a quantitative, label-free 3D imaging method. It is based on the scalar formalism, which limits its application to isotropic samples. We achieve imaging of the birefringence of 3D objects through a reformulation of ODT based on vector diffraction theory. The off-diagonal components of the RI tensor reconstruction convey additional information that is not available in either conventional scalar ODT or 2D polarization microscopy. Finally, we show experimental reconstructions of 3D objects with a polarization-sensitive contrast metric quantitatively displaying the true birefringence of the samples.</DIV></BLOCKQUOTE>

<DIV align="left"><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:anC-K-6lZLwC"><STRONG>Tomographic diffraction microscopy of birefringence</A>.</STRONG> &nbsp;2021 (cannot access)</DIV>
<DIV align="left">&nbsp;</DIV>
<DIV><A href="https://www.osapublishing.org/abstract.cfm?uri=COSI-2020-CF4C.5"><STRONG>Deep learning approach for solving the missing cone problem in optical diffraction tomography</A>. </STRONG>&nbsp;Joowon Lim, Ahmed B. Ayoub, and Demetri Psaltis. &nbsp;CF4C.5 Computational Optical Sensing and Imaging (COSI) 2020</DIV>
<BLOCKQUOTE>
<DIV>Optical diffraction tomography (ODT) produces three dimensional distribution of refractive index (RI) by measuring scattering fields at various angles. Although the distribution of RI index is highly informative, due to the missing cone problem stemming from the limited-angle acquisition of holograms, reconstructions have very poor resolution along axial direction compared to the horizontal imaging plane. To solve this issue, here we present a novel unsupervised deep learning framework, which learns the probability distribution of missing projection views through optimal transport driven cycleGAN. Experimental results show that missing cone artifact in ODT can be significantly resolved by the proposed method.</DIV></BLOCKQUOTE>

<DIV><A href="https://www.osapublishing.org/abstract.cfm?uri=optica-2-6-517"><STRONG>Learning Approach to Optical Tomography</A>. </STRONG>&nbsp;Ulugbek S Kamilov, Ioannis N Papadopoulos, Morteza H Shoreh, Alexandre Goy, Cedric Vonesch, Michael Unser, Demetri Psaltis, Ulugbek S. Kamilov <A href="https://scholar.google.com/citations?hl=en&user=3qYUSDwAAAAJ&view_op=list_works&sortby=pubdate"><FONT size="1">&gt;&gt;</FONT></A> Optica 2, 517–522 (2015).</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:D8YlkiboQ7MC"><STRONG>Three-dimensional tomography of red blood cells using deep learning</STRONG></A> 2020</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=-CVR2h8AAAAJ&sortby=pubdate&citation_for_view=-CVR2h8AAAAJ:wtggH7XvzjUC"><STRONG>MaxwellNet: Physics-driven deep neural network training based on Maxwell’s equations</A>.</STRONG> &nbsp;J Lim, D Psaltis APL Photonics 2021</DIV>
<BLOCKQUOTE>
<DIV>Maxwell’s equations govern light propagation and its interaction with matter. Therefore, the solution of Maxwell’s equations using computational electromagnetic simulations plays a critical role in understanding light–matter interaction and designing optical elements. Such simulations are often time-consuming, and recent activities have been described to replace or supplement them with trained deep neural networks (DNNs). Such DNNs typically require extensive, computationally demanding simulations using conventional electromagnetic solvers to compose the training dataset. In this paper, we present a novel scheme to train a DNN that solves Maxwell’s equations speedily and accurately without relying on other computational electromagnetic solvers. Our approach is to train a DNN using the residual of Maxwell’s equations as the physics-driven loss function for a network that finds the electric field given the</DIV>
<DIV><A href="https://aip-prod-cdn.literatumonline.com/journals/content/app/2022/app.2022.7.issue-1/5.0071616/20220109/suppl/supplementary_material_revised_2nd.pdf?b92b4ad1b4f274c70877518613abb28b424615f7925cf55ec9c2cb79b9a9dfcd69a8987b50485f1f25a8d2d0ec0e0c3337439fc953b7749a8807487592ecfac898bd64d85aa2c9ea51dc5aad7653ad11bc3ef1376645de70b8905995f16812f0ac0c65e112129140d862361578af476a9ad0625f09a0fb9411b64184437d107160451a0f87602dc4a615eaec4eef28971ed0a71db77b6e7193da81a6c2"><STRONG>Supplimental</STRONG></A></STRONG></DIV>
<DIV>Incorps U-Net !!</DIV>
<DIV>&nbsp;</DIV>
<DIV><IMG src="images\0000@2666_2666-412687c78f3dd1bb.png" border="0"></DIV></BLOCKQUOTE>

<DIV><A href="https://www.photoniques.com/articles/photon/abs/2020/05/photon2020104p34/photon2020104p34.html"><STRONG>Optical neural networks: the 3D connection</STRONG></A></STRONG></DIV>

<!-- Note [2666] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2666] -->


<!-- Begin note[1280] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [1280] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Dual-polarized light-field imaging micro-system via a liquid-crystal microlens array&nbsp;</td>
  <td id="date">6/10/2021 11:02 AM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">PLM; Birefringence3D; </span><span id="auto">Web Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [1280] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [1280] content begin -->


<H1><FONT size="2">Dual-polarized light-field imaging micro-system via a liquid-crystal microlens array for direct three-dimensional observation <A href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-26-4-4035&id=381501">&gt;&gt;</A></FONT></H1>
<H1><FONT size="2">&nbsp;Y. Lei, Q. Tong, Z. Xin, D. Wei, X. Zhang, J. Liao, H. Wang, and C. Xie,</FONT></H1>
<H1><FONT size="2">“Three dimensional measurement with an electrically tunable focused plenoptic camera,”</FONT></H1>
<H1><FONT size="2">Rev. Sci. Instrum. 88</STRONG>(3), 033111 (2017). [<A href="http://dx.doi.org/10.1063/1.4979027" target="_blank">CrossRef</A>] &nbsp;&nbsp;[<A href="http://www.ncbi.nlm.nih.gov/pubmed/28372436" target="_blank">PubMed</A>] &nbsp;</FONT></H1>
<H1>&nbsp;</H1>
<H1><FONT size="2">S. H. Lin, L. S. Huang, C. H. Lin, and C. T. Kuo, “Polarization-independent and fast tunable microlens array based on blue phase liquid crystals,” Opt. Express 22</STRONG>(1), 925–930 (2014). [<A href="http://dx.doi.org/10.1364/OE.22.000925" target="_blank">CrossRef</A>] &nbsp;&nbsp;[<A href="http://www.ncbi.nlm.nih.gov/pubmed/24515052" target="_blank">PubMed</A>] &nbsp;</FONT></H1>

<!-- Note [1280] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[1280] -->


<!-- Begin note[2677] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2677] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Horstmeyer, Duke, Computational Optics Lab&nbsp;</td>
  <td id="date">4/30/2022 2:39 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf; Birefringence3D; </span><span id="auto">Web Clips; Images; Images:Images with Printed Text</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2677] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2677] content begin -->


<H2>Horstmeyer, Duke, Computational Optics Lab</H2>
<P><A href="https://horstmeyer.pratt.duke.edu/people/roarke-horstmeyer">Dr. Roarke Horstmeyer</A>,</P>
<BLOCKQUOTE>
<P><A href="http://horstmeyer.pratt.duke.edu">http://horstmeyer.pratt.duke.edu/</A></P>
<P>ML Course: <A href="https://github.com/deepimaging">deepimaging</A>/<A href="https://github.com/deepimaging/deepimaging.github.io">deepimaging.github.io</A></P>
<P><A href="https://deepimaging.io">deepimaging.io</A> &nbsp;<A href="https://horstmeyer.pratt.duke.edu">Computational Optics Lab</A> &nbsp;<A href="https://www.ramonaoptics.com">https://www.ramonaoptics.com/</A></P></BLOCKQUOTE>

<P><A href="http://horstmeyer.pratt.duke.edu/publications/convolutional-neural-networks-teach-microscopes-how-image">Convolutional neural networks that teach microscopes how to image</A>. &nbsp;R. Horstmeyer, R. Y. Chen, B. Kappes, and B. Judkewitz. &nbsp;2017</P>
<P>Questions for Kevin</P>
<BLOCKQUOTE>
<P>Diffraction Tomography for 3D refractive index<BR>Jones matrix imaging<BR>Tensorial tomographic differential phase-contrast</P></BLOCKQUOTE>

<DIV><A href="https://arxiv.org/abs/2204.11397"><STRONG>Tensorial tomographic differential phase-contrast microscopy.</A> &nbsp;...Kevin C. Zhou, ... , Roarke Horstmeyer &nbsp;2022 [pdf]</STRONG></DIV>
<BLOCKQUOTE>
<H1><FONT size="2">We report Tensorial Tomographic Differential Phase-Contrast microscopy (T2DPC), a quantitative label-free tomographic imaging method for simultaneous measurement of phase and anisotropy. T2DPC extends differential phase-contrast microscopy, a quantitative phase imaging technique, to highlight the vectorial nature of light. The method solves for permittivity tensor of anisotropic samples from intensity measurements acquired with a standard microscope equipped with an LED matrix, a circular polarizer, and a polarization-sensitive camera.</FONT></H1></BLOCKQUOTE>

<P><STRONG>[!!] <A href="https://opg.optica.org/boe/fulltext.cfm?uri=boe-13-3-1457&id=469435">Quantitative Jones matrix imaging using vectorial Fourier ptychography</STRONG></A>, &nbsp;X. Dai et al., &quot; Biomedical Optics Express (2022).</P>
<BLOCKQUOTE>
<P>This paper presents a microscopic imaging technique that uses variable-angle illumination to recover the complex polarimetric properties of a specimen at high resolution and over a large field-of-view. The approach extends Fourier ptychography, which is a synthetic aperture-based imaging approach to improve resolution with phaseless measurements, to additionally account for the vectorial nature of light. After images are acquired using a standard microscope outfitted with an LED illumination array and two polarizers, our vectorial Fourier ptychography (vFP) algorithm solves for the complex 2x2 Jones matrix of the anisotropic specimen of interest at each resolved spatial location. We introduce a new sequential Gauss-Newton-based solver that additionally jointly estimates and removes polarization-dependent imaging system aberrations. We demonstrate effective vFP performance by generating large-area (29 mm2), high-resolution (1.24 μm full-pitch) reconstructions of sample absorption, phase, orientation, diattenuation, and retardance for a variety of calibration samples and biological specimens.</P>
<P>‘This limits our attention in this work to polarization-dependent eﬀects along <EM>x </EM>and <EM>y</EM>, although future work may extend analysis to examine thicker specimens and eﬀects along <EM>z</EM>.’</P>
<P><IMG src="images\0000@2677_2677-41268823e397530f.png" border="0"></P></BLOCKQUOTE>

<P>[!!] <A href="https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-9-12872"><STRONG>Diffraction tomography with a deep image prior</A> &nbsp;</STRONG>Kevin C. Zhou, <EM>Roarke Horstmeyer.</EM> Opt. Express <STRONG>28</STRONG>(9) 12872-12896 (2020)</P>
<BLOCKQUOTE>
<P>We present a tomographic imaging technique, termed Deep Prior Diffraction Tomography (DP-DT), to reconstruct the 3D refractive index (RI) of thick biological samples at high resolution from a sequence of low-resolution images collected under angularly varying illumination. DP-DT processes the multi-angle data using a phase retrieval algorithm that is extended by a deep image prior (DIP), which reparameterizes the 3D sample reconstruction with an untrained, deep generative 3D convolutional neural network (CNN). We show that DP-DT effectively addresses the missing cone problem, which otherwise degrades the resolution and quality of standard 3D reconstruction algorithms. As DP-DT does not require pre-captured data or pre-training, it is not biased towards any particular dataset. Hence, it is a general technique that can be applied to a wide variety of 3D samples, including scenarios in which large datasets for supervised training would be infeasible or expensive.</P>
<P>We further demonstrate the generality of DP-DT, using two different scattering models, the first Born and multi-slice models.</P>
<P>[Deep image prior (DIP)... ] &nbsp;Directly minimizing L can be problematic, given the effects of the missing cone. The DIP is a recently-presented, data-independent method to assist with a large variety of inverse image reconstruction problems without supervised training [34,50–54]. It is an untrained regularization technique that reparameterizes the reconstruction target in the spatial domain as the output of a deep generative CNN that uses pseudorandom noise as input (see Fig. 11 in Appendix A for the architecture we used). After initializing the CNN with pseudorandom noise, DIP optimization then proceeds to update CNN weights to minimize loss, as opposed to directly optimizing the reconstruction target (Smodel). Here, we hypothesize that the DIP’s resistance to unnatural images extends to the third dimension and can help eliminate missing cone artifacts in diffraction tomography.</P>
<P><A href="https://github.com/kevinczhou">kevinczhou</A>/<A href="https://github.com/kevinczhou/deep-prior-diffraction-tomography">deep-prior-diffraction-tomography</A>.</P></BLOCKQUOTE>

<DIV><STRONG>&quot;<A href="https://arxiv.org/abs/2202.11837">Computational 3D microscopy with optical coherence refraction tomography</A>,&quot;</STRONG> K. C. Zhou, R. P. McNabb, R. Qian, S. Degan, A. Dhalla, S. Farsiu, and J. A. Izatt, Optica 9, 593-601 (2022)</DIV>
<BLOCKQUOTE>
<P>5D plenoptic datasets</P>
<P><A href="https://github.com/kevinczhou/3d-ocrt">https://github.com/kevinczhou/3d-ocrt</A></P></BLOCKQUOTE>

<P>X. Yao et al., &quot;<A href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-30-2-1745&id=467320">Increasing a microscope’s effective field of view via overlapped imaging and machine learning</A>,&quot; Optics Express (2022).</P>
<P><A href="https://www.ncbi.nlm.nih.gov/pubmed/24921553">Aperture-scanning Fourier ptychography for 3D refocusing and super-resolution macroscopic imaging.</A> &nbsp;Dong S, Horstmeyer R, Shiradkar R, Guo K, Ou X, Bian Z, Xin H, Zheng G., &nbsp;Opt Express. 2014 Jun 2;22(11):13586-99. doi: 10.1364/OE.22.013586.</P>
<H1><FONT size="2">Mesoscopic photogrammetry with an unstabilized phone camera </FONT><A href="https://arxiv.org/abs/2012.06044"><FONT size="1">&gt;&gt;</FONT></A></FONT></H1>

<!-- Note [2677] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2677] -->


<!-- Begin note[2701] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2701] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Barbastathis, MIT&nbsp;</td>
  <td id="date">5/2/2022 2:13 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf; Birefringence3D; </span><span id="auto">Web Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2701] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2701] content begin -->


<H2>Barbastathis, MIT</H2>
<P><A href="http://optics.mit.edu">Welcome to 3D Optical Systems Group at MIT! | 3D Optical ...</A></P>
<P>George Barbastathis<A href="https://scholar.google.com/citations?hl=en&user=4rsJDwUAAAAJ&view_op=list_works&sortby=pubdate">&gt;&gt;</A> GoogScholar</P>
<DIV><A href="https://becominghuman.ai/using-ai-for-computational-imaging-applications-8ff390307e65">Using AI For Computational Imaging Applications - Becoming ...</A></DIV>
<P><A href="https://on-demand.gputechconf.com/gtc/2018/presentation/S8242-Yang-Juntao-paper.pdf">Deep Learning for Computational Science and Engineering</A></P>
<P><STRONG>On the use of deep learning for computational imaging. &nbsp;</STRONG>George Barbastathis, Aydogan Ozcan, and Guohai Situ, Optica, 6(8), 921-943 (2019). <A href="https://www.osapublishing.org/optica/abstract.cfm?uri=optica-6-8-921">link</A></P>
<P><A href="https://www.spiedigitallibrary.org/profile/George.Barbastathis-12394">George Barbastathis</A> &quot;On the use of machine learning for solving computational imaging problems&quot;, Proc. SPIE 11249, Quantitative Phase Imaging VI, 112490B (14 February 2020); <A href="https://doi.org/10.1117/12.2554397" target="_blank">https://doi.org/10.1117/12.2554397</A></P>
<P><A href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-28-16-24152&id=434003">On the interplay between physical and content priors in deep learning for computational imaging.</A> Mo Deng, Shuai Li, Zhengyun Zhang, Iksung Kang, Nicholas X. Fang, and George Barbastathis <A href="https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-28-16-24152&id=434003">&gt;&gt;</A></P>
<P><A href="https://ui.adsabs.harvard.edu/#abs/2020arXiv200710734K/abstract">Limited-angle tomographic reconstruction of dense layered objects by dynamical machine learning. </A>Barbastathis &nbsp;2020</P>
<BLOCKQUOTE>
<P>The number of measurements needed for tomographic reconstruction was largely reduced with the use of Deep Neural Network. Therefore, high-resolution 3D reconstruction was possible with only 20 degree of scanning. <A href="http://optics.mit.edu/research">&gt;&gt;</A></P>
<P>where the collection of raw images from multiple angles is viewed analogously to a dynamical system driven by the object-dependent forward scattering operator. The sequence index in angle of illumination plays the role of discrete time in the dynamical system analogy. Thus, the imaging problem turns into a problem of nonlinear system identification, which also suggests dynamical learning as better fit to regularize the reconstructions.</P></BLOCKQUOTE>

<DIV><STRONG>[!!] High-resolution limited-angle phase tomography of dense layered objects using deep neural networks.</STRONG> &nbsp;Alexandre Goy, Girish Rughoobur, Shuai Li, Kwabena Arthur, Akintunde Akinwande, and George Barbastathis, &nbsp;Proceedings of the National Academy of Sciences, 116 (40) 19848-19856 (2019). <A href="https://www.pnas.org/content/116/40/19848.short">link</A></DIV>
<BLOCKQUOTE>
<P><A name="_Hlk84337875">a DNN trained only on synthetic data can be used to successfully reconstruct physical samples disjoint from the synthetic training set. Thus, the need for producing a large number of physical examples for training is ameliorated. The method is generally applicable to tomography with electromagnetic or other types of radiation at all bands.</A></P></BLOCKQUOTE>

<DIV><A href="https://www.nature.com/articles/s41377-020-0302-3"><STRONG>Phase imaging with an untrained neural network</A>.</STRONG> &nbsp;<A href="https://www.nature.com/articles/s41377-020-0302-3#auth-7">George Barbastathis</A> et al &nbsp;<A href="https://www.nature.com/lsa">Light: Science &amp; Applications</A> volume 9, Article number: 77 (2020)</DIV>
<BLOCKQUOTE>
<P>it is possible to experimentally recover an image with an untrained neural network that is built by combining a conventional artificial network such as U-Net with a real-world physical model that represents the image formation physics; we call the resulting model PhysenNet.</P>
<H3><FONT size="2">U-Net based</FONT></H3>
<DIV>Most of the neural networks proposed so far for computational imaging (CI) in optics employ a supervised training strategy, and thus need a large training set to optimize their weights and biases. Setting aside the requirements of environmental and system stability during many hours of data acquisition, in many practical applications, it is unlikely to be possible to obtain sufficient numbers of ground-truth images for training. Here, we propose to overcome this limitation by <STRONG>incorporating into a conventional deep neural network a complete physical model that represents the process of image formation. </STRONG>The most significant advantage of the resulting physics-enhanced deep neural network (PhysenNet) is that it can be used without training beforehand, thus eliminating the need for tens of thousands of labeled data. We take single-beam phase imaging as an example for demonstration. We experimentally show that one needs only to feed PhysenNet a single diffraction pattern of a phase object, and it can automatically optimize the network and eventually produce the object phase through the interplay between the neural network and the physical model. This opens up a new paradigm of neural network design, in which the concept of incorporating a physical model into a neural network can be generalized to solve many other CI problems.</DIV>
<DIV>...</DIV>
<DIV>Although we have demonstrated PhysenNet only for a use case of 2D phase retrieval, it is, in principle, also applicable for 3D objects provided that a multi-projection technique such as tomography is used to collect the data. In these cases, there should be multiple mapping functions Hi, where i = 1,2, …, N denotes the number of projections, that relate the measured intensity Ii to the 3D object function in the ith view. These functions Hi should be implemented to represent the associated physics, and objective function (6) should accordingly be generalized to</DIV>
<DIV><IMG src="images\0002@2701_0001@2701_0000@1321_0000@506_0000@97147_0000@97148_412682eb96c2f073-6b39.png" border="0"></DIV>
<DIV>&nbsp;</DIV>
<DIV>On Synthetic Training:</DIV>
<DIV>Although a training set can be created through numerical modeling of the image formation physics, the mapping function learned in such a case works well only for test images that are similar to those in the training set, resulting in good generalization only within the set of objects with the same priors used during training.</DIV></BLOCKQUOTE>

<H2><FONT size="2">[!!] <A href="https://arxiv.org/abs/2204.03703" target="_self">Physics-assisted Generative Adversarial ...</A> Network for X-Ray Tomography &nbsp;2022</FONT></H2>
<BLOCKQUOTE>
<P>X-ray tomography is capable of imaging the interior of objects in three dimensions non-invasively, with applications in biomedical imaging, materials science, electronic inspection, and other fields. The reconstruction process can be an ill-conditioned inverse problem, requiring regularization to obtain satisfactory reconstructions. Recently, deep learning has been adopted for tomographic reconstruction. Unlike iterative algorithms which require a distribution that is known a priori, deep reconstruction networks can learn a prior distribution through sampling the training distributions. In this work, we develop a Physics-assisted Generative Adversarial Network (PGAN), a two-step algorithm for tomographic reconstruction. In contrast to previous efforts, our PGAN utilizes maximum-likelihood estimates derived from the measurements to regularize the reconstruction with both known physics and the learned prior. Synthetic objects with spatial correlations are integrated circuits (IC) from a proposed model CircuitFaker. Compared with maximum-likelihood estimation, PGAN can reduce the photon requirement with limited projection angles to achieve a given error rate. We further attribute the improvement to the learned prior by reconstructing objects created without spatial correlations. The advantages of using a prior from deep learning in X-ray tomography may further enable low-photon nanoscale imaging.</P>
<P>...</P>
<P>When prior knowledge is used in an iterative algorithm, the optimization balances minimization of the residual of the simulated measurements from a reconstructed object against minimization of the regularization term. Assumed priors such as sparsity, total variation, and nonlocal similarity priors have have been used extensively in X-ray tomography [14–16]. However, without trial and error, it is not straightforward to choose the appropriate prior and regularization weight for a given set of objects [17]. A prior distribution may also be learned from the dataset itself by a machine learning algorithm. Using a large amount of paired training data, a prior can be determined through exploring the statistical properties of the training distributions, improving<BR>the reconstruction quality.</P></BLOCKQUOTE>

<DIV><STRONG>Solving inverse problems using residual neural networks. &nbsp;</STRONG>Ayan Sinha, Justin Lee, Shuai Li, and George Barbastathis <A href="https://opg.optica.org/viewmedia.cfm?uri=DH-2017-W1A.3&seq=0">&gt;&gt;</A></DIV>
<P>&nbsp;</P>
<P>&nbsp;</P>
<P><A href="https://arxiv.org/abs/1702.08516">Lensless computational imaging through deep learning - arXiv</A></P>
<P><A href="https://dspace.mit.edu/handle/1721.1/122070">Computational imaging through deep learning</A> MI Thesis</P>
<BLOCKQUOTE>
<P>In this thesis, the application of DL approaches in two diﬀerent CI scenarios are investigated: imaging through a glass diﬀuser and quantitative phase retrieval (QPR), where an Imaging through Diﬀuser Network (IDiﬀNet) and a Phase Extraction Neural Network (PhENN) are experimentally demonstrated, respectively. This thesis also studies the inﬂuences of the two main factors that determine the performance of a trained neural network: network architecture (connectivity, network depth, etc) and training example quality (spatial frequency content in particular). Motivated by the analysis of the latter factor, two novel approaches, spectral pre-modulation approach and Learning Synthesis by DNN (LS-DNN) method, are successively proposed to improve the visual qualities of the network outputs. &nbsp;Finally, the LS-DNN enhanced PhENN is applied to a phase microscope to recover the phase of a red blood cell (RBC) sample. Furthermore, through simulation of the learned weak object transfer function (WOTF) and experiment on a star-like phase target, we demonstrate that our network has indeed learned the correct physical model rather than doing something trivial as pattern matching.</P></BLOCKQUOTE>

<DIV><A href="https://www.researchgate.net/publication/350711667_Dynamical_machine_learning_volumetric_reconstruction_of_objects'_interiors_from_limited_angular_views">Dynamical machine learning volumetric reconstruction of objects’ interiors from limited angular views</A></DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://www.researchgate.net/publication/354454865_Roadmap_on_Digital_Holography">Roadmap on Digital Holography</A> &nbsp;Barbastethis</DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Volume Holographic Hyperspectral Imaging <A href="https://independent.academia.edu/GeorgeBarbastathis?swp=tc-au-10150903">George Barbastathis</A> 2004, Applied Optics<A href="https://www.academia.edu/10150903/Volume_Holographic_Hyperspectral_Imaging">&gt;&gt;</A></STRONG></DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>“Learning to synthesize: splitting and recombining low and high spatial frequencies for image recovery,”</STRONG> &nbsp;M. Deng, S. Li, and G. Barbastathis, &nbsp;arXiv preprint arXiv:1811.07945 (2018). spectral</DIV>
<P><A href="https://arxiv.org/abs/1711.06810">Imaging through glass diffusers using densely connected convolutional networks</A> &nbsp;Shuai Li, Mo Deng, Justin Lee, Ayan Sinha, George Barbastathis &nbsp;2017</P>
<BLOCKQUOTE>
<P>Computational imaging through scatter generally is accomplished by first characterizing the scattering medium so that its forward operator is obtained; and then imposing additional priors in the form of regularizers on the reconstruction functional so as to improve the condition of the originally ill-posed inverse problem. In the functional, the forward operator and regularizer must be entered explicitly or parametrically (e.g. scattering matrices and dictionaries, respectively.) However, the process of determining these representations is often incomplete, prone to errors, or infeasible. Recently, deep learning architectures have been proposed to instead learn both the forward operator and regularizer through examples. Here, we propose for the first time, to our knowledge, a convolutional neural network architecture called &quot;IDiffNet&quot; for the problem of imaging through diffuse media and demonstrate that IDiffNet has superior generalization capability through extensive tests with well-calibrated diffusers. We found that the Negative Pearson Correlation Coefficient loss function for training is more appropriate for spatially sparse objects and strong scattering conditions. Our results show that the convolutional architecture is robust to the choice of prior, as demonstrated by the use of multiple training and testing object databases, and capable of achieving higher space-bandwidth product reconstructions than previously reported.</P>
<P><IMG src="images\0000@2701_clip_image002.jpg" border="0" width="419" height="305"></P></BLOCKQUOTE>

<P>&nbsp;</P>
<P><STRONG>The transport of intensity equation for optical path length recovery using partially coherent illumination.</STRONG> &nbsp;J. C. Petruccelli, L. Tian, G. Barbastathis, Opt. Exp. 21, 14430 (2013)</P>
<P><A href="https://dspace.mit.edu/handle/1721.1/67600">High-resolution 3-D refractive index imaging and Its ...</A> Sung, 2011 &nbsp;MIT Thesis</P>
<BLOCKQUOTE>
<P>This thesis presents a theory of 3-D imaging in partially coherent light under a non-paraxial condition. The transmission cross-coefficient (TCC) has been used to characterize partially coherent imaging in a 2- D and 3-D paraxial model. It is shown that the 3-D TCC under the non-paraxial condition is an overlap integral of three hemispheres or 3-D pupil functions. By an inspection of the overlap integral, a simple formula is derived that can be efficiently calculated by multiple applications of the 3-D fast Fourier transform (FFT) instead of a 6-D integral. The theory is applied to phase contrast and differential interference contrast (DIC), and it provides the most rigorous 3-D model that has ever been suggested. Contrast-agent-free microscopy is highly desirable to study the dynamics and physiological activity of various structures in living cells. Refractive index is an intrinsic contrast source, but at the same time it is an important biochemical parameter that is proportional to the concentration of molecules. By measuring the refractive index quantitatively, the alteration of cells under chemicals or drugs as well as their normal physiological activities can be monitored in most native conditions. This thesis presents 3-D optical diffraction tomography (ODT) to retrieve the 3-D refractive index map of a transparent biological sample and applies them to some interesting biological problems: the study of cell growth and monitoring the effect of drugs on multiple myeloma cells. In most practical applications of ODT, the angular coverage of an incident beam is limited due to finite system numerical aperture. The refractive index map reconstructed from the restricted data set suffers from the missing cone artifact: elongation of the reconstructed shape along the optical axis and underestimation of the value of the refractive index. This thesis presents that the missing-cone artifact can be effectively suppressed by applying positivity and piecewise-smoothness constraints in the iterative reconstruction framework. By filling the missing cone, a 3-D refractive index map can be reconstructed from the scattering into the entire 4-[pi]nt solid angle. With the improved accuracy, we attempt to quantify the dry mass of chromosomes in single living cells in their mitotic phase.</P></BLOCKQUOTE>


<!-- Note [2701] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2701] -->


<!-- Begin note[2698] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2698] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Waller&nbsp;</td>
  <td id="date">5/2/2022 2:09 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">GConf; Birefringence3D; </span><span id="auto">Word Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2698] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2698] content begin -->


<H1>Waller</H1>
<H2>Waller, UC Berkley</H2>
<P><A href="https://www.laurawaller.com/publications/laura-waller">Laura Waller » Computational Imaging Lab</A> &amp; <A href="https://people.eecs.berkeley.edu/~kellman">Michael Kellman</A> UC Berkley</P>
<P><A href="https://www.laurawaller.com/publications/laura-waller">https://www.laurawaller.com/publications/laura-waller/</A></P>
<P>laurawaller.com</P>
<P>Phase, Polarization-related...</P>
<DIV><A href="https://doi.org/10.1117/12.2543402"><STRONG>Physics-based learning for measurement diversity in 3D refractive index microscopy</A>,</STRONG> 2020 SPIE</DIV>
<BLOCKQUOTE>
<P>3D refractive index imaging methods usually rely on a weak-scattering approximation that does not allow for thick samples to be imaged accurately. Recent methods such as 3D Fourier ptychographic microscopy (FPM) instead use multiple-scattering models which allow for thicker objects to be imaged. In practice the illumination-side coding of 3D FPM requires redundant information and may produce inaccurate reconstructions for thick samples. Here, we propose augmenting 3D FPM with detection-side coding using a spatial light modulator (SLM) and optimize the SLM coding strategy with physics-based machine learned pupil coding designs that are optimized for 3D reconstructions. We compare our learned designs to random-, defocus-, Zernike aberrations-based pupil codes in simulated and experimental results.</P></BLOCKQUOTE>

<DIV><A href="https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-6-9-1211&id=418751#ref34">High-resolution 3D refractive index microscopy of multiple-scattering samples from intensity images</A>. &nbsp;Waller, 2019</DIV>
<P><A href="https://www.osapublishing.org/josk/abstract.cfm?uri=josk-18-6-691">High-Resolution 3-D Refractive Index Tomography and 2-D ...</A> &nbsp;2014</P>
<P>&nbsp;</P>
<P><A href="https://arxiv.org/abs/2001.09803">Deep Phase Decoder: Self-calibrating phase microscopy with an untrained deep neural network</A>. &nbsp;Emrah Bostan, Reinhard Heckel, Michael Chen, Michael Kellman, Laura Waller. &nbsp;2020 <A href="https://doi.org/10.1364/OPTICA.389314" target="_blank">CrossRef </A><A href="https://scholar.google.com/scholar?as_q=Deep+phase+decoder:+Self-calibrating+phase+microscopy+with+an+untrained+deep+neural+network&as_occt=title&hl=en&as_sdt=0,31" target="_blank">&nbsp;Google Scholar</A></P>
<BLOCKQUOTE>
<P>Deep neural networks have emerged as effective tools for computational imaging including quantitative phase microscopy of transparent samples. To reconstruct phase from intensity, current approaches rely on supervised learning with training examples; consequently, their performance is sensitive to a match of training and imaging settings. Here we propose a new approach to phase microscopy by using an untrained deep neural network for measurement formation, encapsulating the image prior and imaging physics. Our approach does not require any training data and simultaneously reconstructs the sought phase and pupil-plane aberrations by fitting the weights of the network to the captured images. To demonstrate experimentally, we reconstruct quantitative phase from through-focus images blindly (i.e. no explicit knowledge of the aberrations).</P></BLOCKQUOTE>

<P><A href="https://arxiv.org/abs/1808.03571">&quot;Physics-based Learned Design: Optimized Coded-Illumination for Quantitative Phase Imaging.&quot; &nbsp;</A>Michael Kellman, Emrah Bostan, Nicole Repina, Laura Waller. &nbsp;Transactions on Computational Imaging. IEEE, 2019.</P>
<BLOCKQUOTE>
<P>Coded-illumination can enable quantitative phase microscopy of transparent samples with minimal hardware requirements. Intensity images are captured with different source patterns and a non-linear phase retrieval optimization reconstructs the image. The non-linear nature of the processing makes optimizing the illumination pattern designs complicated. Traditional techniques for experimental design (e.g. condition number optimization, spectral analysis) consider only linear measurement formation models and linear reconstructions. Deep neural networks (DNNs) can efficiently represent the non-linear process and can be optimized over via training in an end-to-end framework. However, DNNs typically require a large amount of training examples and parameters to properly learn the phase retrieval process, without making use of the known physical models. Here, we aim to use both our knowledge of the physics and the power of machine learning together. We develop a new data-driven approach to optimizing coded-illumination patterns for a LED array microscope for a given phase reconstruction algorithm. Our method incorporates both the physics of the measurement scheme and the non-linearity of the reconstruction algorithm into the design problem. This enables efficient parameterization, which allows us to use only a small number of training examples to learn designs that generalize well in the experimental setting without retraining. We show experimental results for both a well-characterized phase target and mouse fibroblast cells using coded-illumination patterns optimized for a sparsity-based phase reconstruction algorithm. Our learned design results using 2 measurements demonstrate similar accuracy to Fourier Ptychography with 69 measurements.</P></BLOCKQUOTE>

<DIV><A href="https://www.osapublishing.org/abstract.cfm?URI=3D-2017-DW2F.2">Modeling light propagation in 3D phase objects</A></DIV>
<BLOCKQUOTE>
<P>Accurate and fast simulation of light propagating through 3D biological cells is important to the development of new computational imaging systems. We compare the finite-difference time-domain (FDTD), multislice, first Born approximation, and series-expanded Born (SEAGLE) simulation methods.</P></BLOCKQUOTE>

<DIV><STRONG>Multi-layer Born multiple-scattering model for 3D phase microscopy. &nbsp;</STRONG>Michael Chen, David Ren, Hsiou-Yuan Liu, Shwetadwip Chowdhury, and Laura Waller &nbsp;<A href="https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-7-5-394&id=431219">&gt;&gt;</A></DIV>
<P><A href="https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-2-2-104&id=310823">3D intensity and phase imaging from light field measurements in an LED array microscope</A>. &nbsp;Lei Tian and Laura Waller &nbsp;2015</P>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<P><A href="https://www.osapublishing.org/abstract.cfm?uri=oe-28-22-32266"><STRONG>Roadmap on 3D integral imaging: sensing, processing, and ...</A> Laura Waller,..., Gordon Wetzstein, &nbsp;2020</STRONG></P>
<DIV><STRONG>Single-shot diffuser-encoded light field imaging. &nbsp;&nbsp;Ng, Waller. &nbsp;2016 IEEE &nbsp;<A href="https://ieeexplore.ieee.org/document/7492880">Go to source web page&gt;&gt;</A></STRONG></DIV>
<BLOCKQUOTE>
<DIV>We capture 4D light field data in a single 2D sensor image by encoding spatio-angular information into a speckle field (causticpattern) through a phase diffuser. Using wave-optics theory and a coherent phase retrieval method, we calibrate the system by measuring the diffuser surface height from through-focus images. Wave-optics theory further informs the design of system geometry such that a purely additive ray-optics model is valid. Light field reconstruction is done using nonlinear matrix inversion methods, including ℓ1 minimization. We demonstrate a prototype system and present empirical results of 4D light field reconstruction and computational refocusing from a single diffuser-encoded 2D image.</DIV></BLOCKQUOTE>

<P><A href="https://people.eecs.berkeley.edu/~kellman/topics/pbld2018/BAIR2018_final.pdf">Data-Driven Design for Computational Imaging</A></P>
<P><A href="http://www.opticsexpress.org/abstract.cfm?URI=oe-29-13-20913">Untrained networks for compressive lensless photography</A>. &nbsp;Kristina Monakhova; Vi Tran; Grace Kuo; Laura Waller. &nbsp;Opt. Express, 29 (13), pp. 20913–20929, 2021.</P>
<BLOCKQUOTE>
<P><IMG src="images\0000@2698_clip_image001.jpg" border="0" width="168" height="168" alt="Article Cover"></P>
<P><A href="https://github.com/Waller-Lab/UDN">https://github.com/Waller-Lab/UDN/</A></P></BLOCKQUOTE>

<P><A href="https://arxiv.org/abs/2003.05551">Memory-efficient Learning for Large-scale Computational Imaging</A> 2020</P>
<BLOCKQUOTE>
<P>Critical aspects of computational imaging systems, such as experimental design and image priors, can be optimized through deep networks formed by the <STRONG>unrolled iterations of classical model-based reconstructions</STRONG> (termed<STRONG> physics-based</STRONG> networks). However, for real-world large-scale inverse problems, computing gradients via backpropagation is infeasible due to memory limitations of graphics processing units. In this work, we propose a memory-efficient learning procedure that exploits the reversibility of the network's layers to enable data-driven design for large-scale computational imaging systems. We demonstrate our method on a small-scale <STRONG>compressed sensing</STRONG> example, as well as two large-scale real-world systems: multi-channel magnetic resonance imaging and super-resolution optical microscopy.</P></BLOCKQUOTE>

<P><A href="https://doi.org/10.1038/s41598-019-43845-9">Low-cost, sub-micron resolution, wide-field computational microscopy using opensource hardware</A></P>
<DIV><A href="https://arxiv.org/abs/2005.13531"><STRONG><FONT size="3">How to do Physics-based Learning</A> </FONT>&nbsp;Kellman, Lustig, Waller, 2020</STRONG></DIV>
<BLOCKQUOTE>
<DIV>The goal of this tutorial is to explain step-by-step how to implement physics-based learning for the rapid prototyping of a computational imaging system. We provide a basic overview of physics-based learning, the construction of a physics-based network, and its reduction to practice. Specifically, we advocate exploiting the auto-differentiation functionality twice, once to build a physics-based network and again to perform physics-based learning. Thus, the user need only implement the forward model process for their system, speeding up prototyping time. We provide an open-source Pytorch implementation of a physics-based network and training procedure for a generic sparse recovery problem.</DIV></BLOCKQUOTE>

<H1><A href="https://github.com/kellman"><FONT size="2">kellman</A>/<A href="https://github.com/kellman/physics_based_learning">physics_based_learning</A></FONT></H1>

<!-- Note [2698] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[2698] -->


<!-- Begin note[1304] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [1304] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Polarization Holographic Microscopy/Ptychographic/Jones Matrix/Tomography&nbsp;</td>
  <td id="date">9/30/2021 1:47 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">PLM; Polarization; Birefringence3D; </span><span id="auto">Web Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [1304] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [1304] content begin -->


<DIV><STRONG>Polarization Holographic Microscopy/</STRONG><FONT size="4">Ptychographic</FONT><STRONG>/Jones Matrix/Tomography</STRONG></DIV>
<DIV>refractive index tensor,</DIV>
<DIV>&nbsp;</DIV>
<DIV align="left"><FONT size="4">Polarization-Sensitive Fourier Ptychographic Microscopy</FONT></DIV>
<DIV align="left">&nbsp;</DIV>
<DIV align="left">&nbsp;</DIV>
<DIV><A href="https://www.semanticscholar.org/paper/Large-Area,-High-Resolution-Birefringence-Imaging-Song-Kim/51180eed884de8adaef5e5d79203bc872ec9c854"><STRONG>Large-Area, High-Resolution Birefringence Imaging with Polarization-Sensitive Fourier Ptychographic Microscopy</A>.</STRONG> &nbsp;<A href="https://www.semanticscholar.org/author/Seungri-Song/49768358">Seungri Song</A>, <A href="https://www.semanticscholar.org/author/Jeongsoo-Kim/2109216530">Jeongsoo Kim</A>, <A href="https://www.semanticscholar.org/author/Sunwoong-Hur/2036420805">Sunwoong Hur</A>, <A href="https://www.semanticscholar.org/author/Jaewoo-Song/2144346449">Jaewoo Song</A>, <A href="https://www.semanticscholar.org/author/C.-Joo/2679201">C. Joo</A> &nbsp;Materials Science &nbsp;2021</DIV>
<DIV>&nbsp;</DIV>
<DIV>2D, not 3.</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV>&nbsp;</DIV>
<DIV><FONT size="4">Holographic Polarization Microscopy.</FONT></DIV>
<DIV>&nbsp;</DIV>
<DIV><STRONG>Polarization Holographic Microscopy for Extracting Spatio-Temporally Resolved Jones Matrix.</STRONG> &nbsp;Kim, Y.; Jeong, J.; Jang, J.; Kim, M. W.; Park, Y. &nbsp;Optics Express 2012, 20 (9), 9948–9955</DIV>
<BLOCKQUOTE>
<DIV>We present a high-speed holographic microscopic technique for quantitative measurement of polarization light-field, referred to as polarization holographic microscopy (PHM). Employing the principle of common-path interferometry, PHM quantitatively measures the spatially resolved Jones matrix components of anisotropic samples with only two consecutive measurements of spatially modulated holograms. We demonstrate the features of PHM with imaging the dynamics of liquid crystal droplets at a video-rate</DIV></BLOCKQUOTE>

<DIV>&nbsp;</DIV>
<DIV><A href="https://pubmed.ncbi.nlm.nih.gov/33408986">Exploiting a holographic polarization microscope for rapid autofocusing and 3D tracking.</A> &nbsp;Che L, Xiao W, Pan F, Ferraro P.Biomed Opt Express. 2020</DIV>
<DIV>&nbsp;</DIV>
<DIV><A href="https://pubmed.ncbi.nlm.nih.gov/33182894">Real-time Jones phase microscopy for studying transparent and birefringent specimens.</A> &nbsp;2020</DIV>
<BLOCKQUOTE>
<DIV>Jiao Y, Kandel ME, Liu X, Lu W, Popescu G.Opt Express. 2020 Nov 9;28(23):34190-34200. doi: 10.1364/OE.397062.PMID: 33182894 [pdf]</DIV>
<DIV>&nbsp;</DIV>
<DIV>Tissue birefringence is an intrinsic marker of potential value for cancer diagnosis. Traditionally, birefringence properties have been studied by using intensity-based formalisms, through the Mueller matrix algebra. On the other hand, the Jones matrix description allows for a direct assessment of the sample's anisotropic response. However, because Jones algebra is based on complex fields, requiring measurements of both phase and amplitude, it is less commonly used. Here we propose a real-time imaging method for measuring Jones matrices by quantitative phase imaging. We combine a broadband phase imaging system with a polarization-sensitive detector to obtain Jones matrices at each point in a megapixel scale image, with near video rate capture speeds. To validate the utility of our approach, we measured standard targets, partially birefringent samples, dynamic specimens, and thinly sliced histopathological tissue.</DIV></BLOCKQUOTE>

<DIV><FONT size="4">Stokes-Mueller imaging</FONT></DIV>
<DIV><A href="https://pubmed.ncbi.nlm.nih.gov/30830559">Polarization-resolved Stokes-Mueller imaging: a review of technology and applications.</A></DIV>
<DIV>K U S, Mahato KK, Mazumder N.Lasers Med Sci. 2019 [pdf]</DIV>
<DIV>&nbsp;</DIV>
<H3><FONT size="2">[!!] <A href="https://ui.adsabs.harvard.edu/#abs/2021arXiv210606118G/abstract">Single-shot quantitative polarization imaging of complex birefringent structure dynamics</A> 2021</FONT></H3>
<H3><FONT size="2">2D, not 3.</FONT></H3>
<BLOCKQUOTE>
<H3><FONT size="2">polarized shearing interference microscopy (PSIM), a single-shot quantitative polarization imaging method, for extracting the retardance and orientation angle of the laser beam transmitting through optically anisotropic specimens with complex structures... fast</FONT></H3></BLOCKQUOTE>


<!-- Note [1304] content end -->

</td></tr>
</table>

</td></tr>
</table>
&nbsp

<!-- End note[1304] -->


<!-- Begin note[2654] -->

<table id="note" cellspacing="0" cellpadding="0" width="100%">

<!-- Notebar [2654] begin -->

<tr><td>
<table id="notebar" cellspacing="0" cellpadding="2" width="100%">
<tr valign="top">
  <td id="title">Birefringence measurement of biological tissue based on polarization-sensitive digital&nbsp;</td>
  <td id="date">4/29/2022 2:24 PM</td>
</tr>
<tr valign="top">
  <td id="categories">
Categories: <span id="manual">Birefringence3D; </span><span id="auto">Web Clips</span>
    &nbsp;</td>
  <td id="author">&nbsp;</td>
</tr>
</table>
</td></tr>

<!-- Notebar [2654] end -->

<tr><td>

<table id="body" cellspacing="0" cellpadding="10" width="100%">
<tr><td>

<!-- Note [2654] content begin -->


<H1><FONT size="2">Birefringence measurement of biological tissue based on polarization-sensitive digital holographic microscopy</FONT></H1>
<UL>
<LI><A href="https://link.springer.com/article/10.1007/s00340-018-7098-3#auth-Jiawen-Wang">Jiawen Wang</A>, &nbsp;<A href="https://link.springer.com/article/10.1007/s00340-018-7098-3#auth-Liang-Dong">Liang Dong</A>,<A href="https://link.springer.com/article/10.1007/s00340-018-7098-3#auth-Haige-Chen">Haige Chen</A> &amp;<A href="https://link.springer.com/article/10.1007/s00340-018-7098-3#auth-Sujuan-Huang">Sujuan Huang</A> &nbsp;<A href="https://link.springer.com/journal/340"><EM>Applied Physics B</EM></A> <STRONG>volume 124</STRONG>, Article number: 240 (2018)<A href="https://link.springer.com/article/10.1007/s00340-018-7098-3"><FONT size="1">&gt;&gt;</FONT></A></FONT></LI></UL>


<!-- Note [2654] content end -->

</td></tr>
</table>

</td></tr>
</table>

<!-- End note[2654] -->


</BODY>
</HTML>
 